1
00:00:00,000 --> 00:00:02,700
On Monday about ACA reliability.

2
00:00:02,700 --> 00:00:05,280
We'll do a couple
of top hats soon to

3
00:00:05,280 --> 00:00:08,660
review that material if people
chance ask some questions.

4
00:00:08,660 --> 00:00:11,659
Then we're going to be
looking at Spark again.

5
00:00:11,659 --> 00:00:15,745
Spark actually could integrate
pretty closely with KAFKA.

6
00:00:15,745 --> 00:00:17,969
What we've seen so far
is that we can write

7
00:00:17,969 --> 00:00:20,969
KCA producers and consumers
directly in Python code.

8
00:00:20,969 --> 00:00:23,490
It turns out that Spark can also

9
00:00:23,490 --> 00:00:26,570
act as a Python
producer, consumer.

10
00:00:26,570 --> 00:00:27,989
And in a lot of ways,

11
00:00:27,989 --> 00:00:29,709
it'll feel similar to
what we've seen before.

12
00:00:29,709 --> 00:00:31,489
So what we've done before
with Spark is that

13
00:00:31,489 --> 00:00:33,429
we can read data frames from,

14
00:00:33,429 --> 00:00:35,610
say, Park files in HDFS.

15
00:00:35,610 --> 00:00:38,890
Instead of getting our data
frame from a Park file,

16
00:00:38,890 --> 00:00:41,290
in this case, we're going to
get it from a Kafka stream.

17
00:00:41,290 --> 00:00:43,149
And that'll be
interesting because

18
00:00:43,149 --> 00:00:45,469
Kafka Streams are always
getting new messages.

19
00:00:45,469 --> 00:00:46,809
And so we're essentially
going to have

20
00:00:46,809 --> 00:00:50,309
a Kaka stream data frame that
keeps growing over time.

21
00:00:50,309 --> 00:00:52,170
That'll be called a
streaming data frame.

22
00:00:52,170 --> 00:00:53,889
So today, I'm going
to do a few things.

23
00:00:53,889 --> 00:00:55,969
I'm going to start with
some programming examples.

24
00:00:55,969 --> 00:00:58,190
I'll just see how
it works hands.

25
00:00:58,190 --> 00:01:00,849
And then after that, we'll
actually dig in and understand

26
00:01:00,849 --> 00:01:04,490
internally how it works
and the general concepts.

27
00:01:04,490 --> 00:01:06,569
So first ahead over here and do

28
00:01:06,569 --> 00:01:09,350
a top hat about CFC reliability.

29
00:01:09,350 --> 00:01:11,369
So RF is replication factor,

30
00:01:11,369 --> 00:01:13,190
and it's actually about whether

31
00:01:13,190 --> 00:01:15,889
a given piece of
data is committed.

32
00:01:50,670 --> 00:01:54,110
30 seconds left.

33
00:02:28,300 --> 00:02:30,739
Alright, so most
people are saying,

34
00:02:30,739 --> 00:02:32,919
yes, it's committed, but
that's actually not quite.

35
00:02:32,919 --> 00:02:34,939
This data is not committed.

36
00:02:34,939 --> 00:02:39,199
Okay. So let me do a little
bit of review on this.

37
00:02:39,199 --> 00:02:40,199
It seems like this is a point

38
00:02:40,199 --> 00:02:41,959
that is confusing
a lot of people.

39
00:02:41,959 --> 00:02:43,959
We learned about how
replication works

40
00:02:43,959 --> 00:02:45,620
for Cassandra and also ACA.

41
00:02:45,620 --> 00:02:47,559
And they're very different
strategies, right?

42
00:02:47,559 --> 00:02:50,659
So with Cassandra,
we would say, Okay,

43
00:02:50,659 --> 00:02:51,939
these are the three replicas or

44
00:02:51,939 --> 00:02:53,399
how many give piece of data,

45
00:02:53,399 --> 00:02:55,199
and all the replicas
were equal, right?

46
00:02:55,199 --> 00:02:56,480
You would have some
forum, and you have to

47
00:02:56,480 --> 00:02:57,959
write to some number of them,

48
00:02:57,959 --> 00:03:00,839
but it could be any
particular ones, right?

49
00:03:00,839 --> 00:03:02,520
And the thing that was nice

50
00:03:02,520 --> 00:03:03,820
about that it was
highly available.

51
00:03:03,820 --> 00:03:05,799
Any given worker could go down,

52
00:03:05,799 --> 00:03:07,059
it doesn't matter, and we

53
00:03:07,059 --> 00:03:08,859
could still write to the system.

54
00:03:08,859 --> 00:03:10,339
The disadvantage was that

55
00:03:10,339 --> 00:03:11,759
was eventually
consistent, right?

56
00:03:11,759 --> 00:03:13,439
Given that we could just
write to any of them

57
00:03:13,439 --> 00:03:15,319
we want and still call it drud,

58
00:03:15,319 --> 00:03:17,300
we could have cases
where different workers

59
00:03:17,300 --> 00:03:18,879
have different
versions of a row,

60
00:03:18,879 --> 00:03:20,239
maybe none of which is like

61
00:03:20,239 --> 00:03:22,040
the correct or most
current version.

62
00:03:22,040 --> 00:03:23,600
And then we'd have to
see things like, well,

63
00:03:23,600 --> 00:03:25,879
how do we resolve
conflicts, right?

64
00:03:25,879 --> 00:03:27,879
So trade off there,
Cassandra, right?

65
00:03:27,879 --> 00:03:31,319
V available, but we would have
divergent history, right?

66
00:03:31,319 --> 00:03:34,279
ACA is trying something a
little bit different, right?

67
00:03:34,279 --> 00:03:36,459
In the case of ACA,
they're imagining this is

68
00:03:36,459 --> 00:03:37,540
like the center of your company

69
00:03:37,540 --> 00:03:38,920
where all your data lives,

70
00:03:38,920 --> 00:03:41,080
and whatever it
goes through KACA

71
00:03:41,080 --> 00:03:43,320
is populating downstream
data sources,

72
00:03:43,320 --> 00:03:47,010
it's not okay to have that
kind of wishy washy data.

73
00:03:47,010 --> 00:03:48,829
We really need to, you know,

74
00:03:48,829 --> 00:03:50,689
know exactly what
the messages are.

75
00:03:50,689 --> 00:03:52,569
They can't change. Everybody has

76
00:03:52,569 --> 00:03:54,529
to agree about the
order of the messages.

77
00:03:54,529 --> 00:03:55,929
And because of that,

78
00:03:55,929 --> 00:03:59,109
Taka uses leader follower
replication, right?

79
00:03:59,109 --> 00:04:01,290
All the rights are going
through a single leader,

80
00:04:01,290 --> 00:04:04,109
and that leader is going
to be the authority on

81
00:04:04,109 --> 00:04:05,870
what that message
is and what order

82
00:04:05,870 --> 00:04:07,169
the messages are are in, right?

83
00:04:07,169 --> 00:04:08,809
So we don't have all that hand

84
00:04:08,809 --> 00:04:10,529
weavines that we
get with Cassandra.

85
00:04:10,529 --> 00:04:12,530
Now, Kafka is not going to be as

86
00:04:12,530 --> 00:04:15,409
available because if
that leader dies,

87
00:04:15,409 --> 00:04:17,090
it takes a while to
recognize that it's

88
00:04:17,090 --> 00:04:19,330
dead and elect a
different leader.

89
00:04:19,330 --> 00:04:21,649
Cassandra, there's
no kind of downtime,

90
00:04:21,649 --> 00:04:24,030
whereas there might
be some downtime for

91
00:04:24,030 --> 00:04:26,509
Kafka. All right, great.

92
00:04:26,509 --> 00:04:27,870
So we have that leader,

93
00:04:27,870 --> 00:04:30,270
and it's getting
all these messages,

94
00:04:30,270 --> 00:04:32,689
and the followers are pulling

95
00:04:32,689 --> 00:04:35,830
those messages down
to themselves, right?

96
00:04:35,830 --> 00:04:37,789
And ideally,

97
00:04:37,789 --> 00:04:39,690
all four of them are going
to have all the messages,

98
00:04:39,690 --> 00:04:40,770
but of course, that's not true,

99
00:04:40,770 --> 00:04:42,370
and there's some lag, right?

100
00:04:42,370 --> 00:04:44,850
Now, some of the replicas might

101
00:04:44,850 --> 00:04:47,710
be in sync or not in
sync at any given time.

102
00:04:47,710 --> 00:04:49,510
And I'm wondering if anybody can

103
00:04:49,510 --> 00:04:52,450
remind me what does it mean
for them to be in sync?

104
00:04:57,910 --> 00:05:01,309
Is that a hand off right
here? Yeah, right here.

105
00:05:02,790 --> 00:05:06,969
Does it have to be I guess.

106
00:05:06,969 --> 00:05:08,789
I'm I'm looking at a replica,

107
00:05:08,789 --> 00:05:11,710
and I want to say, is
this replica and sync?

108
00:05:18,910 --> 00:05:21,449
Yeah, it's fetched
the latest message

109
00:05:21,449 --> 00:05:23,050
from the leader in a
reasonable time frame.

110
00:05:23,050 --> 00:05:24,070
And you can kind
of configure that.

111
00:05:24,070 --> 00:05:26,490
There's a little bit
of flexibility there.

112
00:05:26,490 --> 00:05:28,229
When a follower goes

113
00:05:28,229 --> 00:05:30,434
the leader says, give
me some messages.

114
00:05:30,434 --> 00:05:32,440
There's a limit, how, how

115
00:05:32,440 --> 00:05:33,959
big of a batch we want
to send it on, right?

116
00:05:33,959 --> 00:05:35,540
So it's possible I
give me some messages,

117
00:05:35,540 --> 00:05:36,840
and it may or may not

118
00:05:36,840 --> 00:05:39,540
be fetching all the
way to the end, right?

119
00:05:39,540 --> 00:05:41,660
Right? If I kind of
am falling behind,

120
00:05:41,660 --> 00:05:43,099
then I might not
fetch to the end.

121
00:05:43,099 --> 00:05:46,519
And so what it's looking for
is, did we fetch to the end?

122
00:05:46,519 --> 00:05:48,660
And when was the last
time we did that, right?

123
00:05:48,660 --> 00:05:50,780
That's some measure of
how fresh it is, right?

124
00:05:50,780 --> 00:05:52,500
And you can set the threshold,

125
00:05:52,500 --> 00:05:54,840
but if it did that within
some time frame, we'll say,

126
00:05:54,840 --> 00:05:57,520
Okay, this follower is

127
00:05:57,520 --> 00:05:59,780
sink it's keeping
up reasonably well.

128
00:05:59,780 --> 00:06:01,520
That's another kind of
interesting thing about

129
00:06:01,520 --> 00:06:04,319
Kafka because from the
outside perspective,

130
00:06:04,319 --> 00:06:05,709
when I'm looking at a node,

131
00:06:05,709 --> 00:06:07,080
And it's not responding to me.

132
00:06:07,080 --> 00:06:08,879
It's really hard for me to
tell if it's just running

133
00:06:08,879 --> 00:06:11,199
slow or it's behind or
it's actually down.

134
00:06:11,199 --> 00:06:13,440
And at some level, it doesn't
really matter, right?

135
00:06:13,440 --> 00:06:15,380
And so it's just like, Well,
it's either in sync or not.

136
00:06:15,380 --> 00:06:18,020
Okay, so that's fine.
I can identify that.

137
00:06:18,020 --> 00:06:19,580
And the leader knows

138
00:06:19,580 --> 00:06:21,660
which ones are in sync
and which ones are not.

139
00:06:21,660 --> 00:06:23,520
And it uses that
information about who's in

140
00:06:23,520 --> 00:06:26,164
sync and who's not for
a couple of purposes.

141
00:06:26,164 --> 00:06:28,410
One is, if we don't have

142
00:06:28,410 --> 00:06:30,909
enough followers
that are in sync,

143
00:06:30,909 --> 00:06:32,310
I mean, we can't keep up, right?

144
00:06:32,310 --> 00:06:34,229
We don't want to keep taking
more and more messages

145
00:06:34,229 --> 00:06:35,890
that the followers
can't keep up with.

146
00:06:35,890 --> 00:06:37,749
So at some point, we're
going to tell producers,

147
00:06:37,749 --> 00:06:40,249
hey, back off, don't send
me as many messages.

148
00:06:40,249 --> 00:06:43,930
Okay? And that's where men
sinc comes into play, right?

149
00:06:43,930 --> 00:06:45,990
If we don't have
enough inc followers,

150
00:06:45,990 --> 00:06:48,369
we'll tell the producers
stop sending us messages

151
00:06:48,369 --> 00:06:50,830
for a while until they catch up.

152
00:06:50,830 --> 00:06:52,170
That's what Men sinc does.

153
00:06:52,170 --> 00:06:53,750
It really has to do
with availability

154
00:06:53,750 --> 00:06:55,469
and also durability.

155
00:06:55,469 --> 00:06:59,910
Okay. So if we're okay with,

156
00:06:59,910 --> 00:07:03,070
too, And our data is on two.

157
00:07:03,070 --> 00:07:06,149
Why can we not say this
data is committed?

158
00:07:06,149 --> 00:07:08,950
Why is the leader not okay
with that? Here's why.

159
00:07:08,950 --> 00:07:10,229
The leader is well aware,

160
00:07:10,229 --> 00:07:11,510
it could die at any moment and

161
00:07:11,510 --> 00:07:13,049
be replaced with a follower.

162
00:07:13,049 --> 00:07:14,889
And if the leader wants

163
00:07:14,889 --> 00:07:17,129
to say that this
message is committed,

164
00:07:17,129 --> 00:07:19,369
any follower that can replace

165
00:07:19,369 --> 00:07:22,570
the leader better also
have that message, right?

166
00:07:22,570 --> 00:07:24,790
Because if the leader dies
and a follower takes over

167
00:07:24,790 --> 00:07:27,170
that doesn't have that
message, that message is gone.

168
00:07:27,170 --> 00:07:28,309
Because that new follower,

169
00:07:28,309 --> 00:07:30,790
which becomes a leader
is the authority, right?

170
00:07:30,790 --> 00:07:32,410
And so that's why
we have this rule.

171
00:07:32,410 --> 00:07:36,570
Data is not committed until
it's on all sync followers.

172
00:07:36,570 --> 00:07:38,830
Because any no that's in

173
00:07:38,830 --> 00:07:41,799
sync could become
the next leader.

174
00:07:41,799 --> 00:07:44,200
Right? Our data is not safe
until every candidate for

175
00:07:44,200 --> 00:07:47,280
leader agrees about what
that message is, right?

176
00:07:47,280 --> 00:07:48,880
And that's why we have
different thresholds.

177
00:07:48,880 --> 00:07:50,019
The mite Isc is whether or not

178
00:07:50,019 --> 00:07:52,220
we're telling
producers to back off.

179
00:07:52,220 --> 00:07:54,480
In terms of whether it's
committed, it's like,

180
00:07:54,480 --> 00:07:56,279
well is it on every
insinc replica.

181
00:07:56,279 --> 00:07:57,579
So do people have
have any questions

182
00:07:57,579 --> 00:07:59,660
about pica replication?

183
00:07:59,660 --> 00:08:07,579
Yeah, right here. Mm hmm.

184
00:08:13,780 --> 00:08:17,440
So you're saying that o.

185
00:08:17,440 --> 00:08:19,560
So you're saying the state in
the system changes, right?

186
00:08:19,560 --> 00:08:20,840
So this moment in time, it's

187
00:08:20,840 --> 00:08:23,140
not committed, and
then what happens?

188
00:08:27,420 --> 00:08:29,540
Right.

189
00:08:37,980 --> 00:08:40,460
Mm hmm. Yeah. Okay, great.

190
00:08:40,460 --> 00:08:42,939
So there's a few things
that could happen, right?

191
00:08:42,939 --> 00:08:45,720
One. So in this state,

192
00:08:45,720 --> 00:08:48,339
one is that the
sync replica that

193
00:08:48,339 --> 00:08:50,980
doesn't have the message
could grab the message,

194
00:08:50,980 --> 00:08:53,219
and then we'll say,
now it's committed.

195
00:08:53,219 --> 00:08:55,059
That's one thing
that could happen.

196
00:08:55,059 --> 00:08:58,340
The other thing is that we've
classified this worker as

197
00:08:58,340 --> 00:09:02,019
in sync because it relatively
recently was keeping up,

198
00:09:02,019 --> 00:09:03,859
and it's possible
that it recently had

199
00:09:03,859 --> 00:09:06,320
a problem and it's start
to be not in sync soon.

200
00:09:06,320 --> 00:09:08,859
So the other possibilities
that this Esache replica

201
00:09:08,859 --> 00:09:10,319
could way too long
and that we could

202
00:09:10,319 --> 00:09:11,900
reclassify it as not Esache.

203
00:09:11,900 --> 00:09:13,480
And at that point,
we could say it's

204
00:09:13,480 --> 00:09:14,840
committed because it's

205
00:09:14,840 --> 00:09:16,319
two because there are

206
00:09:16,319 --> 00:09:19,220
only two Ed psyche.
Does that make sense?

207
00:09:20,420 --> 00:09:23,820
How do we decide if
it's committed or not?

208
00:09:23,820 --> 00:09:27,599
Yeah. So the leader
is the one that will

209
00:09:27,599 --> 00:09:30,920
decide if it's committed
because when it is committed,

210
00:09:30,920 --> 00:09:32,059
we communicate that back with

211
00:09:32,059 --> 00:09:34,079
an acknowledgment to
the producer, right?

212
00:09:34,079 --> 00:09:35,059
And the leader is the one

213
00:09:35,059 --> 00:09:36,759
that's that acknowledgment back.

214
00:09:36,759 --> 00:09:38,940
And so it turns out
that the leader knows

215
00:09:38,940 --> 00:09:41,180
who is in sake because it knows

216
00:09:41,180 --> 00:09:45,900
when these other followers
are fetching messages.

217
00:09:45,900 --> 00:09:47,680
Not only that, like In

218
00:09:47,680 --> 00:09:49,440
addition to knowing
which ones are sake,

219
00:09:49,440 --> 00:09:51,460
the leader needs to
communicate that elsewhere.

220
00:09:51,460 --> 00:09:52,660
Because if the leader dies,

221
00:09:52,660 --> 00:09:54,860
then there's a separate
election system

222
00:09:54,860 --> 00:09:56,999
that needs to chose one
that's sake, right?

223
00:09:56,999 --> 00:09:59,279
So that leader is writing
to a separate system.

224
00:09:59,279 --> 00:10:00,279
Could be somebody called

225
00:10:00,279 --> 00:10:01,840
Zookeeper which we
won't get into,

226
00:10:01,840 --> 00:10:03,479
could be somebody
called Craft, right?

227
00:10:03,479 --> 00:10:05,700
There's some external system
that's highly reliable.

228
00:10:05,700 --> 00:10:07,000
We'll just assume that's there.

229
00:10:07,000 --> 00:10:08,619
And that leader knows who's in

230
00:10:08,619 --> 00:10:10,579
sake and to try to
communicate it there, right?

231
00:10:10,579 --> 00:10:12,280
So the leader will
decide who's sake.

232
00:10:12,280 --> 00:10:13,859
Do that makes sens?

233
00:10:25,050 --> 00:10:28,630
The one that is lagging
is not in sync,

234
00:10:28,630 --> 00:10:30,750
and for it to become
insync again,

235
00:10:30,750 --> 00:10:33,989
that means we have to fetch
the very last message.

236
00:10:33,989 --> 00:10:35,129
Therefore, it will also have

237
00:10:35,129 --> 00:10:36,509
this message that
we're worried about.

238
00:10:36,509 --> 00:10:37,890
I saw some other adds up, too,

239
00:10:37,890 --> 00:10:39,130
so I want to make sure I get

240
00:10:39,130 --> 00:10:40,770
some time for
everybody right here.

241
00:10:40,770 --> 00:10:45,549
These What is the
purpose of s replicas?

242
00:10:45,549 --> 00:10:46,850
You could imagine they

243
00:10:46,850 --> 00:10:48,009
could have built the
system without that,

244
00:10:48,009 --> 00:10:48,790
and we would still have

245
00:10:48,790 --> 00:10:51,285
the same guarantees
around committed.

246
00:10:51,285 --> 00:10:53,420
What would happen
that case is that

247
00:10:53,420 --> 00:10:56,000
producers will just keep
sending us messages,

248
00:10:56,000 --> 00:10:58,059
and we won't be
acknowledging them because

249
00:10:58,059 --> 00:11:01,559
the Nyc replicas are never
really catching up, right?

250
00:11:01,559 --> 00:11:04,859
So what would happen if we
didn't have men sync replicas?

251
00:11:04,859 --> 00:11:06,719
Eventually, all the
followers would

252
00:11:06,719 --> 00:11:10,599
be out of sync if
they can't keep up.

253
00:11:10,599 --> 00:11:13,139
And then because we don't
have any men, we'll just say,

254
00:11:13,139 --> 00:11:16,320
well, there's the leader
whose Ny by definition.

255
00:11:16,320 --> 00:11:18,099
Therefore, it's
committed on the leader.

256
00:11:18,099 --> 00:11:19,000
But then we would have gotten

257
00:11:19,000 --> 00:11:20,699
ourselves in a bad case because

258
00:11:20,699 --> 00:11:22,599
there's no candidate
to take over

259
00:11:22,599 --> 00:11:25,399
if the leader dies, right?

260
00:11:41,020 --> 00:11:43,720
I mean, I guess our
data is more durable

261
00:11:43,720 --> 00:11:45,699
if we have a higher
men in sync, right?

262
00:11:45,699 --> 00:11:47,159
Because if we don't
have any men in sync,

263
00:11:47,159 --> 00:11:49,180
and we just have everything
else is the same,

264
00:11:49,180 --> 00:11:52,739
then and the producers are
sending messages too fast,

265
00:11:52,739 --> 00:11:54,820
then eventually every faller

266
00:11:54,820 --> 00:11:56,460
will be considered not in sync,

267
00:11:56,460 --> 00:11:59,080
and so only the leader
will be in sync.

268
00:11:59,080 --> 00:12:05,040
That's a bad situation
because If the leader dies,

269
00:12:05,040 --> 00:12:08,599
there's no follower who
can take over, right?

270
00:12:08,599 --> 00:12:10,719
Because nobody's in sync, right?

271
00:12:10,719 --> 00:12:12,099
And so it would be
better if we're

272
00:12:12,099 --> 00:12:13,539
following behind to
tell the producers,

273
00:12:13,539 --> 00:12:15,179
I can't accept these
messages right

274
00:12:15,179 --> 00:12:18,739
now because we don't have
enough followers keeping up.

275
00:12:18,739 --> 00:12:21,200
Assuming these
followers are not dead,

276
00:12:21,200 --> 00:12:22,979
they're just trying to
like running behind.

277
00:12:22,979 --> 00:12:25,520
Then if we stop taking
new messages for a while,

278
00:12:25,520 --> 00:12:28,219
they will get a chance
to actually catch up,

279
00:12:28,219 --> 00:12:30,860
and then we'll be able
to maintain some number

280
00:12:30,860 --> 00:12:34,580
of replicas that could
take over as the leader.

281
00:12:34,580 --> 00:12:36,120
Yeah, hopefully
that makes sense.

282
00:12:36,120 --> 00:12:37,379
Yeah, lots of great questions.

283
00:12:37,379 --> 00:12:42,939
People have other questions.
Right. Cool. So it's

284
00:12:42,939 --> 00:12:44,780
a very different system,
right than Cassandra.

285
00:12:44,780 --> 00:12:47,199
It's interesting to
compare them. All right.

286
00:12:47,199 --> 00:12:48,579
One other one I
want to do is about

287
00:12:48,579 --> 00:12:51,499
this word called item potents.

288
00:12:51,499 --> 00:12:53,640
And so I just kind
of a fun question

289
00:12:53,640 --> 00:12:55,699
about house sitting, right?

290
00:12:55,699 --> 00:12:57,060
When people give you directions,

291
00:12:57,060 --> 00:12:58,779
the kind of operation
they're asking you to do

292
00:12:58,779 --> 00:13:01,559
may or may not be item potent.

293
00:13:37,050 --> 00:13:40,089
15 seconds left.

294
00:13:59,090 --> 00:14:01,769
All right, so people are saying

295
00:14:01,769 --> 00:14:03,749
empty the cat's litter
box, which is true, right?

296
00:14:03,749 --> 00:14:05,290
So if you empty the
cat's litter box,

297
00:14:05,290 --> 00:14:07,430
the state of it is that
it's empty or clean.

298
00:14:07,430 --> 00:14:09,109
And if you repeat those steps

299
00:14:09,109 --> 00:14:10,670
again in an already
clean litter box,

300
00:14:10,670 --> 00:14:12,910
well, then it will
still be clean.

301
00:14:12,910 --> 00:14:13,850
What about the other ones?

302
00:14:13,850 --> 00:14:15,649
Somebody said some
people are saying

303
00:14:15,649 --> 00:14:18,030
decrease the b two
degrees, right?

304
00:14:18,030 --> 00:14:21,719
So if my Friend tells me
that I decrease it by two,

305
00:14:21,719 --> 00:14:23,379
and they tell me, again,

306
00:14:23,379 --> 00:14:24,859
I decrease it by two.

307
00:14:24,859 --> 00:14:26,500
The outcome is different

308
00:14:26,500 --> 00:14:28,259
than if I had done
it once, right?

309
00:14:28,259 --> 00:14:30,539
The state of the temperature
in the house will

310
00:14:30,539 --> 00:14:33,239
be different if I do that one
time or many times, right?

311
00:14:33,239 --> 00:14:34,860
That's what you want
to ask yourself,

312
00:14:34,860 --> 00:14:36,100
what is the ending state?

313
00:14:36,100 --> 00:14:37,899
And will the ending
state be different

314
00:14:37,899 --> 00:14:39,960
depending on whether I do
it one time or many times?

315
00:14:39,960 --> 00:14:41,760
Any questions about that
one? I see that one through

316
00:14:41,760 --> 00:14:44,490
some people the dog would
be very happy here,

317
00:14:44,490 --> 00:14:46,709
if it's just cheap
until you get an act,

318
00:14:46,709 --> 00:14:49,570
the dog keeps getting
more food. Alright, cool.

319
00:14:49,570 --> 00:14:51,689
So a little bit review
from last time.

320
00:14:51,689 --> 00:14:53,589
One other thing I want
to talk about briefly is

321
00:14:53,589 --> 00:14:56,650
that some people have been
having issues with their VM.

322
00:14:56,650 --> 00:14:58,989
There's some directions on
here about how to troubleshoot

323
00:14:58,989 --> 00:15:01,430
like if it's VPN, who to ask.

324
00:15:01,430 --> 00:15:02,289
I think some people have

325
00:15:02,289 --> 00:15:03,869
been a little bit
stuck on this one.

326
00:15:03,869 --> 00:15:05,429
So I just want to show
what this means, right?

327
00:15:05,429 --> 00:15:07,630
So if your VM is non responsive,

328
00:15:07,630 --> 00:15:09,549
you might have to reboot it.

329
00:15:09,549 --> 00:15:12,030
And on my laptop right now,

330
00:15:12,030 --> 00:15:13,690
if I go and I say,
like, you know,

331
00:15:13,690 --> 00:15:15,129
shut down or restart,

332
00:15:15,129 --> 00:15:16,510
what the file system will do is

333
00:15:16,510 --> 00:15:18,110
the file system is going to
have some data in memory.

334
00:15:18,110 --> 00:15:19,510
I try to write that
out to desk before

335
00:15:19,510 --> 00:15:21,090
it reboots. So I
don't lose anything.

336
00:15:21,090 --> 00:15:24,550
Now, If my computer,
for whatever reason,

337
00:15:24,550 --> 00:15:26,989
is like hung up, it might not
be able to write that out,

338
00:15:26,989 --> 00:15:28,510
and I just will not be
able to rebot normally.

339
00:15:28,510 --> 00:15:30,389
In that case of what I do, I
just hit the power button.

340
00:15:30,389 --> 00:15:32,610
Same thing with your VM. If
your VM's not responsive,

341
00:15:32,610 --> 00:15:33,649
you probably have to
do the equivalent

342
00:15:33,649 --> 00:15:34,790
of just like a hard reset,

343
00:15:34,790 --> 00:15:36,409
then rather a clean
reboot, right?

344
00:15:36,409 --> 00:15:37,610
So when we do the reboot,

345
00:15:37,610 --> 00:15:38,930
right, we wanted
the reset option.

346
00:15:38,930 --> 00:15:40,569
And so if you go to
the Proxmx thing,

347
00:15:40,569 --> 00:15:41,989
which I linked to, you know,

348
00:15:41,989 --> 00:15:43,090
I can see my VM here.

349
00:15:43,090 --> 00:15:44,549
And by the way, I can't
see yours, right?

350
00:15:44,549 --> 00:15:45,490
So people are going to have to

351
00:15:45,490 --> 00:15:46,589
do this on their own a bit.

352
00:15:46,589 --> 00:15:47,990
I can you know, look at

353
00:15:47,990 --> 00:15:49,769
your computer, but I
can't do it for you.

354
00:15:49,769 --> 00:15:51,530
I'm going to click on
this. If I right click,

355
00:15:51,530 --> 00:15:53,210
they don't actually
show the reset option.

356
00:15:53,210 --> 00:15:54,669
They hide it a little bit.

357
00:15:54,669 --> 00:15:56,189
But if I double click on it,

358
00:15:56,189 --> 00:15:58,389
then I can go ahead
and I can do a reset,

359
00:15:58,389 --> 00:15:59,910
and that should work even if

360
00:15:59,910 --> 00:16:01,599
it's kind of in some
weird hung state.

361
00:16:01,599 --> 00:16:02,989
Alright, C. You know,

362
00:16:02,989 --> 00:16:04,809
just show me your laptop if that

363
00:16:04,809 --> 00:16:06,870
somehow is not working.
I'd be curious.

364
00:16:06,870 --> 00:16:09,249
Alright, so let's jump
into the new stuff.

365
00:16:09,249 --> 00:16:13,009
Um, I have somewhat of a
similar set up as last time,

366
00:16:13,009 --> 00:16:15,889
right, I have my
Kafka client here.

367
00:16:15,889 --> 00:16:20,469
I'm going to make a stream or
a topic called animals JSN.

368
00:16:20,469 --> 00:16:21,349
Very similar last time.

369
00:16:21,349 --> 00:16:22,770
Last time we used
protocol buffers,

370
00:16:22,770 --> 00:16:25,770
and there's nothing wrong
with that except with Spark.

371
00:16:25,770 --> 00:16:28,049
It's very hard to parse
protocol buffers unless you

372
00:16:28,049 --> 00:16:30,850
pay databricks for their
non open source version.

373
00:16:30,850 --> 00:16:32,870
And so I'm just
going to use JSN for

374
00:16:32,870 --> 00:16:34,949
our formats of protocol
buffers for this one.

375
00:16:34,949 --> 00:16:38,709
And I have an anti producer
down here like before.

376
00:16:38,709 --> 00:16:42,109
The beach is a random
letter like ABCDE.

377
00:16:42,109 --> 00:16:45,169
I'm choosing a random animal
every second I do this.

378
00:16:45,169 --> 00:16:47,150
And so here I just
have to send it off.

379
00:16:47,150 --> 00:16:48,449
And I'm going to do that now.

380
00:16:48,449 --> 00:16:53,280
I'm going to say,
producer that send,

381
00:16:53,280 --> 00:16:55,640
and I'll say the
topic is going to be

382
00:16:55,640 --> 00:16:59,219
animal animals J SN.

383
00:16:59,580 --> 00:17:03,279
All right. And then the key
will equal something and

384
00:17:03,279 --> 00:17:07,179
the value will equal
something as well. All right.

385
00:17:07,179 --> 00:17:08,680
Well, for the key,

386
00:17:08,680 --> 00:17:10,340
it's going to be based
on the beach again,

387
00:17:10,340 --> 00:17:11,459
but it has to be bytes.

388
00:17:11,459 --> 00:17:14,619
So I will just convert that
to UTF eight like last time.

389
00:17:14,619 --> 00:17:16,199
And for the value,

390
00:17:16,199 --> 00:17:18,639
I'm going to take a dictionary

391
00:17:18,639 --> 00:17:20,040
with all the information into

392
00:17:20,040 --> 00:17:21,419
it and convert it
to bytes, right?

393
00:17:21,419 --> 00:17:23,960
So All the information

394
00:17:23,960 --> 00:17:26,959
is the beach and also
the animal in question.

395
00:17:26,959 --> 00:17:30,839
And so what I'm going to do
is I'm going to dump it.

396
00:17:30,839 --> 00:17:34,060
Dump it to a string,
right, JSN dump.

397
00:17:34,060 --> 00:17:37,099
And then I'm going to convert
that to bytes, right?

398
00:17:37,099 --> 00:17:40,480
So UTF eight like B four.

399
00:17:40,480 --> 00:17:41,760
And then I can just pass

400
00:17:41,760 --> 00:17:44,320
those values in
here. Alright, cool.

401
00:17:44,320 --> 00:17:45,760
And so I'm going
to kick this off

402
00:17:45,760 --> 00:17:47,520
as a thread running
in the background.

403
00:17:47,520 --> 00:17:48,800
And I don't really
punt anything.

404
00:17:48,800 --> 00:17:49,940
So I'm just going to assume it's

405
00:17:49,940 --> 00:17:51,320
working unless I get some error,

406
00:17:51,320 --> 00:17:52,859
if I'm missing data later,

407
00:17:52,859 --> 00:17:54,619
then I'll come back and
look at it more closely.

408
00:17:54,619 --> 00:17:56,360
Anyway, I'm generating JSN data.

409
00:17:56,360 --> 00:17:57,979
It's also nice to see
an example of how

410
00:17:57,979 --> 00:18:00,144
you could have JSN dictionaries.

411
00:18:00,144 --> 00:18:02,369
Be your message format, right?

412
00:18:02,369 --> 00:18:04,350
Alright, that thing is
running in the background.

413
00:18:04,350 --> 00:18:07,589
Now, if we were doing
things the old way, right?

414
00:18:07,589 --> 00:18:09,089
My previous examples, I would go

415
00:18:09,089 --> 00:18:11,050
right like a Python consumer,

416
00:18:11,050 --> 00:18:12,750
I would pull things
off, and maybe I'd

417
00:18:12,750 --> 00:18:15,309
try to count by beach
or count by animal.

418
00:18:15,309 --> 00:18:17,589
What I may do now is I'm may use

419
00:18:17,589 --> 00:18:20,289
Spark as my consumer, right?

420
00:18:20,289 --> 00:18:21,929
And if you want to do that,

421
00:18:21,929 --> 00:18:26,829
then you have to add this
Jars packages thing here.

422
00:18:26,829 --> 00:18:28,630
Remember that all this
spark stuff is based

423
00:18:28,630 --> 00:18:30,590
on the JVM with the JVM.

424
00:18:30,590 --> 00:18:32,590
You can have code resources
that are in these ars,

425
00:18:32,590 --> 00:18:34,389
as like Java archive or
something like that.

426
00:18:34,389 --> 00:18:35,629
Anyway, I'm going
to do that and it's

427
00:18:35,629 --> 00:18:37,049
going to automatically
download it from

428
00:18:37,049 --> 00:18:38,849
something called Ma Maven

429
00:18:38,849 --> 00:18:40,509
is kind of like the
PEP repository.

430
00:18:40,509 --> 00:18:43,350
Anyway, I'm going to get a
spark session where I can use

431
00:18:43,350 --> 00:18:45,609
libraries that let me access

432
00:18:45,609 --> 00:18:47,889
CP data, right? So I've
already done that.

433
00:18:47,889 --> 00:18:49,819
All that stuff happens.

434
00:18:49,819 --> 00:18:51,889
And then we're were
to do down here is

435
00:18:51,889 --> 00:18:53,750
we're to do a spark dot read.

436
00:18:53,750 --> 00:18:55,209
And what I want to do is I

437
00:18:55,209 --> 00:18:56,650
want to read from
that Kafka stream.

438
00:18:56,650 --> 00:18:58,430
So we've seen before
we could say something

439
00:18:58,430 --> 00:19:00,669
like Park and then I don't know,

440
00:19:00,669 --> 00:19:02,389
some more stuff and
load it back in.

441
00:19:02,389 --> 00:19:03,610
I might put a path there.

442
00:19:03,610 --> 00:19:04,930
That's how we would
do it before.

443
00:19:04,930 --> 00:19:06,789
Now our format
that we're pulling

444
00:19:06,789 --> 00:19:09,070
from is actually
going to be Kafka.

445
00:19:09,070 --> 00:19:11,329
And there's going to
be no path, right?

446
00:19:11,329 --> 00:19:13,869
Because they're not files.
They're just Kafka streams.

447
00:19:13,869 --> 00:19:15,809
Instead, I can put
some options in

448
00:19:15,809 --> 00:19:19,389
here to specify how I'm
going to find that thing.

449
00:19:19,389 --> 00:19:20,750
This a little bit long.

450
00:19:20,750 --> 00:19:23,050
So I'm going to just
break it up a bit,

451
00:19:23,050 --> 00:19:28,049
and We'll just space that out.

452
00:19:28,049 --> 00:19:30,409
Now, remember that
I could easily have

453
00:19:30,409 --> 00:19:33,189
a bunch of different Kafka
clusters in my company.

454
00:19:33,189 --> 00:19:34,729
And so one of the things
we have to do is we

455
00:19:34,729 --> 00:19:36,529
have to say, which 1:00
A.M. I interested in.

456
00:19:36,529 --> 00:19:38,109
And the way I do that is I say

457
00:19:38,109 --> 00:19:41,150
Kafka dot bootstrap servers.

458
00:19:41,150 --> 00:19:43,949
And then I have to
specify what it is.

459
00:19:43,949 --> 00:19:45,369
And this is really analogous

460
00:19:45,369 --> 00:19:49,109
to like how producers or
consumers normally work.

461
00:19:49,109 --> 00:19:52,529
Like up here, we'd say bootstrap
servers equal a broker,

462
00:19:52,529 --> 00:19:55,669
and my broker is in
the same container.

463
00:19:55,669 --> 00:19:57,489
It's on Local Host 90 92.

464
00:19:57,489 --> 00:19:59,269
And I'll just come down
here and I'm going to say,

465
00:19:59,269 --> 00:20:01,229
Okay, well, this is
the Kafka cluster.

466
00:20:01,229 --> 00:20:03,754
And then down here, I
can do a subscribe.

467
00:20:03,754 --> 00:20:05,359
Just like any consumer would,

468
00:20:05,359 --> 00:20:07,220
you can subscribe to topics,

469
00:20:07,220 --> 00:20:10,020
and I'm going to
subscribe to animals.

470
00:20:10,020 --> 00:20:15,499
Jason, And so I'm going
to get that whole thing,

471
00:20:15,499 --> 00:20:18,219
and let me just make

472
00:20:18,219 --> 00:20:20,599
a little bit more
room here. All right.

473
00:20:20,599 --> 00:20:23,899
Great. And I'm going to put
that in something called DF.

474
00:20:23,899 --> 00:20:26,419
And when I look at DF,

475
00:20:26,419 --> 00:20:28,179
what I'm may see is that it

476
00:20:28,179 --> 00:20:29,959
doesn't matter what
stream I chose from,

477
00:20:29,959 --> 00:20:31,280
I always has the same things.

478
00:20:31,280 --> 00:20:33,320
It has the key of the message.

479
00:20:33,320 --> 00:20:34,719
It has the value of the message.

480
00:20:34,719 --> 00:20:35,959
And get anything out of here,

481
00:20:35,959 --> 00:20:38,839
the real work will
be, can I take this,

482
00:20:38,839 --> 00:20:42,079
which is bytes and extract it,

483
00:20:42,079 --> 00:20:43,999
so I get a few different
columns there.

484
00:20:43,999 --> 00:20:45,659
Another thing that
we're going to see,

485
00:20:45,659 --> 00:20:47,470
is that if I look at the count,

486
00:20:47,470 --> 00:20:50,099
Of this. I go to run some jobs,

487
00:20:50,099 --> 00:20:52,720
and we'll tell me how many,

488
00:20:52,720 --> 00:20:55,679
I guess there's 181
messages there so far.

489
00:20:55,679 --> 00:20:59,100
If I run it again instead
of 181, it will be 187.

490
00:20:59,100 --> 00:21:01,119
Right? So every time I run
it, I can just, you know,

491
00:21:01,119 --> 00:21:03,780
get a snapshot of
that Kafka stream,

492
00:21:03,780 --> 00:21:06,079
which is growing forever
and ever, right?

493
00:21:06,079 --> 00:21:07,579
Alright, so I have all of that

494
00:21:07,579 --> 00:21:09,680
there. That's fine and well.

495
00:21:09,680 --> 00:21:13,159
If I want to say,

496
00:21:13,680 --> 00:21:16,920
I'm going to say
data frame do limit.

497
00:21:16,920 --> 00:21:18,260
I'll just do it like the first

498
00:21:18,260 --> 00:21:20,500
five and convert them to Pandas.

499
00:21:20,500 --> 00:21:23,400
And so I'm going to
see it down here,

500
00:21:23,400 --> 00:21:25,119
these values, they kind of look

501
00:21:25,119 --> 00:21:26,960
like like lists of integers.

502
00:21:26,960 --> 00:21:31,439
They're not. That's the way
they show you bytes, right?

503
00:21:31,439 --> 00:21:34,840
A byte could be a number 0-255.

504
00:21:34,840 --> 00:21:36,799
And so I see while there's a
bunch of numbers like that.

505
00:21:36,799 --> 00:21:39,799
These are bytes. My job is
to extract it back so I can

506
00:21:39,799 --> 00:21:41,499
actually see what animal

507
00:21:41,499 --> 00:21:43,699
and what beach is
involved, right?

508
00:21:43,699 --> 00:21:46,939
And so a lot of the
stuff that we do, right?

509
00:21:46,939 --> 00:21:50,240
A lot of this processing
is based on functions

510
00:21:50,240 --> 00:21:55,060
under Pi Spark SQL functions.

511
00:21:55,060 --> 00:21:58,180
And I'm a important column
that we've seen before EXPR.

512
00:21:58,180 --> 00:21:59,819
My do another one, which
is going to help us,

513
00:21:59,819 --> 00:22:01,704
which is from JSN.

514
00:22:01,704 --> 00:22:04,010
And so I have this data frame,

515
00:22:04,010 --> 00:22:04,909
and I'm going to try to

516
00:22:04,909 --> 00:22:06,169
parse some stuff out
of there, right?

517
00:22:06,169 --> 00:22:09,230
So I could select, for
example, a couple of columns,

518
00:22:09,230 --> 00:22:10,810
I could select out the key,

519
00:22:10,810 --> 00:22:13,889
and I could select
out the value, right?

520
00:22:13,889 --> 00:22:15,569
That's what I'm most
interested in here.

521
00:22:15,569 --> 00:22:17,150
I could hit those two things.

522
00:22:17,150 --> 00:22:19,209
And if I want to,

523
00:22:19,209 --> 00:22:20,669
I could cast it.

524
00:22:20,669 --> 00:22:22,489
Cast means I'm
changing the type,

525
00:22:22,489 --> 00:22:24,130
and I could do it to a string.

526
00:22:24,130 --> 00:22:26,569
Very fortunately, for me,

527
00:22:27,050 --> 00:22:29,990
by default, it assumes

528
00:22:29,990 --> 00:22:32,150
a UTF eight encoding, if
I go bytes to string.

529
00:22:32,150 --> 00:22:33,029
And in fact, I don't think

530
00:22:33,029 --> 00:22:34,129
there's even a way
to override it.

531
00:22:34,129 --> 00:22:36,430
So a good thing I use
a UTF eight encoding.

532
00:22:36,430 --> 00:22:39,074
And so I can do all that.

533
00:22:39,074 --> 00:22:42,459
Great. And let's
take a look at that.

534
00:22:42,459 --> 00:22:44,300
Maybe let me just do the limit

535
00:22:44,300 --> 00:22:47,339
five again in two Panda so
we can see where we're at.

536
00:22:47,339 --> 00:22:49,599
All right. I can see I'm
already making progress.

537
00:22:49,599 --> 00:22:51,459
I can see, well, this
was a row of data that

538
00:22:51,459 --> 00:22:54,649
came from from where.

539
00:22:54,649 --> 00:22:58,089
Like, way back here, right?
I got this beach and animal.

540
00:22:58,089 --> 00:23:00,889
I wrote it to a message.
That was the value,

541
00:23:00,889 --> 00:23:02,349
which was bytes in the message.

542
00:23:02,349 --> 00:23:03,649
And now I can already see,

543
00:23:03,649 --> 00:23:08,449
and in the Spark side,
I can get that back.

544
00:23:08,449 --> 00:23:10,529
I can get a string
representing that Jason.

545
00:23:10,529 --> 00:23:11,869
Now, it's not everything I want.

546
00:23:11,869 --> 00:23:13,709
I would like to somehow extract

547
00:23:13,709 --> 00:23:14,989
individual things like this and

548
00:23:14,989 --> 00:23:16,410
make them individual columns,

549
00:23:16,410 --> 00:23:18,649
because I actually want
to do analysis over it.

550
00:23:18,649 --> 00:23:21,549
And so what I'm going
to do next is I

551
00:23:21,549 --> 00:23:24,850
will say, check on this here.

552
00:23:24,850 --> 00:23:27,710
I will say from J SN,

553
00:23:27,710 --> 00:23:31,330
on this one, and that will
convert it to these strucks.

554
00:23:31,330 --> 00:23:34,269
And when I do that, I have
to specify some schema,

555
00:23:34,269 --> 00:23:35,870
because I need to say, well,

556
00:23:35,870 --> 00:23:39,109
what types am I
expecting in this thing,

557
00:23:39,109 --> 00:23:41,589
what are the column
names, right?

558
00:23:41,589 --> 00:23:43,670
And so the schema
is just a string,

559
00:23:43,670 --> 00:23:45,470
and some say schema up here.

560
00:23:45,470 --> 00:23:47,130
And what it should
do is it should

561
00:23:47,130 --> 00:23:49,009
specify the names of
the field, right?

562
00:23:49,009 --> 00:23:52,609
I'm going to say Bach
and creature, right?

563
00:23:52,609 --> 00:23:54,349
And I also have to
put the types for it.

564
00:23:54,349 --> 00:23:55,569
The type names come afterwards.

565
00:23:55,569 --> 00:23:57,929
And so in this case,
these are both strings.

566
00:23:57,929 --> 00:24:00,050
I'm to say string, string.

567
00:24:00,050 --> 00:24:02,230
And then I'm going to
pass this in here.

568
00:24:02,230 --> 00:24:04,469
Let's try that. And what

569
00:24:04,469 --> 00:24:06,389
we actually see is I have
a little issue, right?

570
00:24:06,389 --> 00:24:07,550
I'm getting the beach name,

571
00:24:07,550 --> 00:24:10,589
but I don't have creature
because if you look

572
00:24:10,589 --> 00:24:11,989
back here where we
were writing it

573
00:24:11,989 --> 00:24:13,929
before, I called
an animal, right?

574
00:24:13,929 --> 00:24:15,289
So I had a typo, right?

575
00:24:15,289 --> 00:24:16,769
And that's to emphasize

576
00:24:16,769 --> 00:24:19,799
that how we actually
parse this out.

577
00:24:19,799 --> 00:24:21,140
Now, you can imagine that these

578
00:24:21,140 --> 00:24:22,819
different producers over
a different time, right?

579
00:24:22,819 --> 00:24:24,080
It's just running indefinitely,

580
00:24:24,080 --> 00:24:25,399
and people change that code.

581
00:24:25,399 --> 00:24:27,520
They might add stuff
to these dictionaries

582
00:24:27,520 --> 00:24:29,879
or remove it over time, right?

583
00:24:29,879 --> 00:24:31,339
And so what happens if

584
00:24:31,339 --> 00:24:32,939
the schema has
something extra here?

585
00:24:32,939 --> 00:24:35,379
It just fills it in with
the none because maybe it

586
00:24:35,379 --> 00:24:38,100
thinks that I'm supposed to
have a column with that name,

587
00:24:38,100 --> 00:24:40,559
and older messages
didn't have it.

588
00:24:40,559 --> 00:24:42,279
On the flip side, right,

589
00:24:42,279 --> 00:24:44,319
if there is something like,

590
00:24:44,319 --> 00:24:46,359
if it has animal, right in
the actual dictionary and

591
00:24:46,359 --> 00:24:48,519
I don't have animal up
here, just drops it, right?

592
00:24:48,519 --> 00:24:50,789
So it's going to
work regardless why,

593
00:24:50,789 --> 00:24:52,110
I'm not going to get any error,

594
00:24:52,110 --> 00:24:54,429
but it might mismatch, right?

595
00:24:54,429 --> 00:24:55,589
So you have missing data, you'd

596
00:24:55,589 --> 00:24:57,169
want to go back and
double check that.

597
00:24:57,169 --> 00:24:59,230
Right? I'm may say
that as animal.

598
00:24:59,230 --> 00:25:01,809
And now I'm making some
real progress, right?

599
00:25:01,809 --> 00:25:04,729
I can actually see it
looks like a tuple here.

600
00:25:04,729 --> 00:25:07,449
It's actually a structure. It
has names on those fields.

601
00:25:07,449 --> 00:25:10,709
But I can see I'm
making progress, right?

602
00:25:10,709 --> 00:25:13,090
What I may do is I may
give this a new name,

603
00:25:13,090 --> 00:25:15,100
which is just going to be value.

604
00:25:15,100 --> 00:25:18,169
And then the reason why
is that down here I

605
00:25:18,169 --> 00:25:20,989
can select a couple
of things, right?

606
00:25:20,989 --> 00:25:24,510
I can select down
here, both the key.

607
00:25:24,510 --> 00:25:27,170
And then instead of value,
I may say value dot star.

608
00:25:27,170 --> 00:25:28,749
And then all the
entries inside of

609
00:25:28,749 --> 00:25:31,149
those struts are going to
become their own own column.

610
00:25:31,149 --> 00:25:32,689
I'm going to do that. And now I

611
00:25:32,689 --> 00:25:34,409
say I've actually made a
lot of progress, right?

612
00:25:34,409 --> 00:25:36,390
I can d from that
JSN dictionary,

613
00:25:36,390 --> 00:25:37,769
and I actually can
get some columns,

614
00:25:37,769 --> 00:25:39,069
and I could analyze it.

615
00:25:39,069 --> 00:25:41,349
If I wanted to, I can start
counting the beaches or

616
00:25:41,349 --> 00:25:44,590
start counting the
animals. All right, cool.

617
00:25:44,590 --> 00:25:47,849
Any questions so far
about all of that kind of

618
00:25:47,849 --> 00:25:54,400
consumer parsing stuff? Right.

619
00:25:54,400 --> 00:25:58,979
Cool. So when I look at
this thing, this DF.

620
00:26:01,700 --> 00:26:03,859
You know, actually,
what I may do is

621
00:26:03,859 --> 00:26:05,220
I may call this animals.

622
00:26:05,220 --> 00:26:06,879
I may put it in there, and

623
00:26:06,879 --> 00:26:08,419
then I'll just
take a look at it.

624
00:26:08,419 --> 00:26:10,699
Animals, Dot limit, two pandas.

625
00:26:10,699 --> 00:26:12,780
When I look at animals,

626
00:26:12,780 --> 00:26:16,100
it's a data frame, but I
can say, is it streaming?

627
00:26:16,100 --> 00:26:18,639
And the answer is false, right?

628
00:26:18,639 --> 00:26:20,674
They have a special
case for that.

629
00:26:20,674 --> 00:26:22,370
Right now what I'm doing is it's

630
00:26:22,370 --> 00:26:24,230
a Kafka stream, and it's
constantly growing,

631
00:26:24,230 --> 00:26:26,249
but I read it at a single
point in time and I get

632
00:26:26,249 --> 00:26:28,529
a static data frame,
that's not changing.

633
00:26:28,529 --> 00:26:30,629
What I want to do is
have a new kind of

634
00:26:30,629 --> 00:26:34,330
data frame that grows
with the Kafka stream.

635
00:26:34,330 --> 00:26:36,169
Alright? And soba
come back here,

636
00:26:36,169 --> 00:26:40,809
and I am going to get my
code that was reading DF.

637
00:26:40,809 --> 00:26:44,989
And it's actually a relatively
simple change, right?

638
00:26:44,989 --> 00:26:48,269
So to get a streaming DF,

639
00:26:48,269 --> 00:26:51,850
change read to read stream.

640
00:26:51,850 --> 00:26:54,329
So I'm just going to put
read stream down here.

641
00:26:54,329 --> 00:26:56,390
Alright. And then at this point,

642
00:26:56,390 --> 00:26:59,680
DF is going to be a streaming.

643
00:26:59,680 --> 00:27:03,449
Data frame. And if I get
animals like this back here,

644
00:27:03,449 --> 00:27:05,870
I'm going to be the
same thing back here.

645
00:27:06,870 --> 00:27:09,650
Let's get these animals,

646
00:27:09,650 --> 00:27:14,229
and animals is also going
to be streaming, right?

647
00:27:14,229 --> 00:27:16,370
If I have a streaming data frame

648
00:27:16,370 --> 00:27:17,529
and I do some
transformations on,

649
00:27:17,529 --> 00:27:19,989
I get a new streaming
data frame.

650
00:27:19,989 --> 00:27:22,569
Okay. And there's may be
different things that we

651
00:27:22,569 --> 00:27:23,970
can do with a
streaming data frame

652
00:27:23,970 --> 00:27:25,190
in a regular data frame.

653
00:27:25,190 --> 00:27:30,469
So, for example, I want to
say how many rows are there.

654
00:27:30,469 --> 00:27:32,869
Like I did before, I
can't do that, right?

655
00:27:32,869 --> 00:27:34,409
When I come down
here and I look,

656
00:27:34,409 --> 00:27:35,989
it says, I need to

657
00:27:35,989 --> 00:27:37,909
do something elses,
right stream thing.

658
00:27:37,909 --> 00:27:39,629
It doesn't make sense, right,

659
00:27:39,629 --> 00:27:42,040
because This is a data frame

660
00:27:42,040 --> 00:27:43,560
that's by definition
constantly growing.

661
00:27:43,560 --> 00:27:46,339
How can I possibly say
what the count of it is?

662
00:27:46,339 --> 00:27:48,239
Right? That's constantly
changing, right?

663
00:27:48,239 --> 00:27:51,579
Okay, so how do we
actually use these things?

664
00:27:51,579 --> 00:27:55,539
The format will be kind of
like this. Where do I say?

665
00:27:55,780 --> 00:27:59,420
Where I go from
source, a data source,

666
00:27:59,420 --> 00:28:01,604
where I do some transformations,

667
00:28:01,604 --> 00:28:04,489
and then we have some sync
where the data ends, right?

668
00:28:04,489 --> 00:28:05,629
And so what we're doing is we're

669
00:28:05,629 --> 00:28:07,269
constantly moving
data from point A to

670
00:28:07,269 --> 00:28:10,369
point B with some transformations
in the middle, right?

671
00:28:10,369 --> 00:28:11,630
And so how do we specify

672
00:28:11,630 --> 00:28:13,270
the data source
of the data sync?

673
00:28:13,270 --> 00:28:14,689
Well, we're going to do
something like this,

674
00:28:14,689 --> 00:28:18,530
where I say spar
dot read Stream.

675
00:28:18,530 --> 00:28:20,249
And then there's going to be

676
00:28:20,249 --> 00:28:21,889
some transformations
in the middle,

677
00:28:21,889 --> 00:28:23,809
possibly or maybe not, right?

678
00:28:23,809 --> 00:28:25,309
And then at the end, where I say

679
00:28:25,309 --> 00:28:28,469
write stream to
someplace else, right?

680
00:28:28,469 --> 00:28:30,349
And so what this will
do is we can say,

681
00:28:30,349 --> 00:28:32,189
this is where the
data is coming from.

682
00:28:32,189 --> 00:28:33,809
This is where the
data is ding to,

683
00:28:33,809 --> 00:28:39,029
and these are all the things
I'm doing in between, right?

684
00:28:39,029 --> 00:28:42,539
Okay. So how could we

685
00:28:42,539 --> 00:28:44,640
actually build some kind

686
00:28:44,640 --> 00:28:46,899
of application based
on this, right?

687
00:28:46,899 --> 00:28:50,079
So say to do build,

688
00:28:50,079 --> 00:28:53,760
build a shark alert
app, for swimmers.

689
00:28:53,760 --> 00:28:55,359
They know not to go
to those beaches.

690
00:28:55,359 --> 00:28:59,519
And I'm trying to say taking
this stream right from Taka.

691
00:28:59,519 --> 00:29:01,660
And then the sync

692
00:29:01,660 --> 00:29:04,160
where the day is ready to
do is just be the screen.

693
00:29:04,160 --> 00:29:07,540
The screen will pop up messages

694
00:29:07,540 --> 00:29:09,180
whenever there's a
shark that's been seen.

695
00:29:09,180 --> 00:29:11,240
They'll tell me where
they've been seen.

696
00:29:11,240 --> 00:29:12,739
And how ma I do this?

697
00:29:12,739 --> 00:29:15,459
All this stuff here like
the Spark Red Stream and

698
00:29:15,459 --> 00:29:17,779
transformations If you look

699
00:29:17,779 --> 00:29:19,079
back here, I've
already done that.

700
00:29:19,079 --> 00:29:20,419
I have the read
stream right here.

701
00:29:20,419 --> 00:29:22,319
That's the source of my data.

702
00:29:22,319 --> 00:29:25,419
I already have some
transformations based on it.

703
00:29:25,419 --> 00:29:27,079
And so if I look at,

704
00:29:27,079 --> 00:29:29,419
like this part here,
I already have that,

705
00:29:29,419 --> 00:29:31,139
and it's called animals, right?

706
00:29:31,139 --> 00:29:35,060
I've given animals as the
name of half my pipeline.

707
00:29:35,060 --> 00:29:38,020
Now, I want to do some
additional transformations

708
00:29:38,020 --> 00:29:39,219
on it before I
send it somewhere,

709
00:29:39,219 --> 00:29:40,699
and I'm going to say dot filter,

710
00:29:40,699 --> 00:29:45,859
and I'm going to say where
the animal is a is a shark.

711
00:29:45,859 --> 00:29:47,960
I do that thing, and
I get a data frame,

712
00:29:47,960 --> 00:29:49,279
I can see a schema on it.

713
00:29:49,279 --> 00:29:52,500
But to really complete it, I
have to do a right stream.

714
00:29:52,500 --> 00:29:54,140
So I'm do that right Stream.

715
00:29:54,140 --> 00:29:55,739
And maybe this is another one

716
00:29:55,739 --> 00:29:57,399
of those ones where I
want to break it up.

717
00:29:57,399 --> 00:29:59,660
Some to say right Stream,

718
00:29:59,660 --> 00:30:01,859
And for this one,

719
00:30:01,859 --> 00:30:03,459
I go to have to say
you the format, right?

720
00:30:03,459 --> 00:30:05,000
I mean, I could do
like CPA, again,

721
00:30:05,000 --> 00:30:07,339
I could be writing
the Sarka art out to,

722
00:30:07,339 --> 00:30:09,799
like, a special topic, and
people could subscribe to it.

723
00:30:09,799 --> 00:30:11,879
In this case, I just want to

724
00:30:11,879 --> 00:30:14,500
send it straight to the
consul, which is my screen.

725
00:30:14,500 --> 00:30:15,859
I just want to see it, right?

726
00:30:15,859 --> 00:30:17,379
Alright. So I'm gonna do that.

727
00:30:17,379 --> 00:30:19,099
And what else, right?

728
00:30:19,099 --> 00:30:20,660
It'll kind of match these things

729
00:30:20,660 --> 00:30:22,259
up and do a few at a time,

730
00:30:22,259 --> 00:30:23,919
right as I have the
messages coming in.

731
00:30:23,919 --> 00:30:26,419
And so I can say what
trigger will do it.

732
00:30:26,419 --> 00:30:29,999
And I'm going to say
processing time.

733
00:30:30,030 --> 00:30:32,449
Equals 5 seconds.

734
00:30:32,449 --> 00:30:34,069
So about every 5
seconds is going to be

735
00:30:34,069 --> 00:30:35,749
giving me some shark alert.

736
00:30:35,749 --> 00:30:38,130
Hopefully, that's enough time
to get out of the water.

737
00:30:38,130 --> 00:30:43,389
Then finally, I have to say
what output mode Do I want,

738
00:30:43,389 --> 00:30:44,729
right? And I'm may say, a pen.

739
00:30:44,729 --> 00:30:47,710
A pen means that I'm writing
to the end of something

740
00:30:47,710 --> 00:30:50,930
and I never go back and
make earlier changes.

741
00:30:50,930 --> 00:30:54,710
So for example, if I have a
new shark that's been seen,

742
00:30:54,710 --> 00:30:56,810
I'm going to pen
some new output,

743
00:30:56,810 --> 00:30:58,709
and that would not make me,

744
00:30:58,709 --> 00:31:01,249
go back and change any of
my previous outputs, right?

745
00:31:01,249 --> 00:31:02,829
A new shark does not make me

746
00:31:02,829 --> 00:31:05,229
change what I said about
previous animals, right?

747
00:31:05,229 --> 00:31:07,709
I'll just have that be

748
00:31:07,709 --> 00:31:09,429
added to the stream,
right? So do that.

749
00:31:09,429 --> 00:31:11,529
And when I do all of this,

750
00:31:11,529 --> 00:31:13,509
I get the data
stream writer thing,

751
00:31:13,509 --> 00:31:15,070
and if I really wanted to start,

752
00:31:15,070 --> 00:31:16,530
then I have to say, start.

753
00:31:16,530 --> 00:31:19,169
And start is going
to return an object,

754
00:31:19,169 --> 00:31:21,050
which is called a streaming

755
00:31:21,050 --> 00:31:22,969
A streaming query, right?

756
00:31:22,969 --> 00:31:24,469
All right. So I'm going
to do that, right?

757
00:31:24,469 --> 00:31:25,829
I did a bunch of this had

758
00:31:25,829 --> 00:31:28,350
that red stream inside if
I did some pre processing,

759
00:31:28,350 --> 00:31:29,689
more pre processing, now I

760
00:31:29,689 --> 00:31:31,449
write stream and it's drawing
somewhere else, right?

761
00:31:31,449 --> 00:31:32,989
So Data is going to
be coming off that

762
00:31:32,989 --> 00:31:36,350
Poka stream onto my stream,

763
00:31:36,350 --> 00:31:37,789
right? I have some batches here.

764
00:31:37,789 --> 00:31:40,210
No sharks, thankfully.
Okay, there's a shark.

765
00:31:40,210 --> 00:31:41,569
Well, lots of sharks all at

766
00:31:41,569 --> 00:31:43,509
the same time and a bunch of
different beaches, right?

767
00:31:43,509 --> 00:31:44,749
Not a good day for swimming.

768
00:31:44,749 --> 00:31:47,269
And that's just ting to keep
running forever, right?

769
00:31:47,269 --> 00:31:49,209
I would be kind of weird
to do this on NOPA, right?

770
00:31:49,209 --> 00:31:50,389
People probably
normally do this like

771
00:31:50,389 --> 00:31:52,429
an adopt Pi file that's
running somewhere,

772
00:31:52,429 --> 00:31:55,710
but I'm just doing it here
for the sake of of example.

773
00:31:55,710 --> 00:31:57,649
All right. So I'm going
to pause there and see.

774
00:31:57,649 --> 00:32:00,830
Do people have any questions
about this streaming job?

775
00:32:03,810 --> 00:32:06,510
Here. Where does this start?

776
00:32:06,510 --> 00:32:08,529
I'm assuming that
before this there

777
00:32:08,529 --> 00:32:10,949
were so many sharks in the Wh,

778
00:32:10,949 --> 00:32:12,869
yeah, that's a really
excellent question, right?

779
00:32:12,869 --> 00:32:14,629
Be You assumption is right,
that there are a lot of

780
00:32:14,629 --> 00:32:16,890
sharks before I even got
to this far in the code.

781
00:32:16,890 --> 00:32:18,969
And by default, it
just starts at the end

782
00:32:18,969 --> 00:32:21,269
of the topic. I will
do an example soon.

783
00:32:21,269 --> 00:32:24,190
How can I guess last
time we saw we could

784
00:32:24,190 --> 00:32:27,710
call S two beginning,
I think it was called.

785
00:32:27,710 --> 00:32:29,369
And they have
something analogous to

786
00:32:29,369 --> 00:32:31,370
that probably a wrap
around that I show.

787
00:32:31,370 --> 00:32:32,709
Yeah, Excellent point. Yeah, all

788
00:32:32,709 --> 00:32:34,975
the points people
have are questions.

789
00:32:34,975 --> 00:32:38,920
Right. Cool. So that
thing's running.

790
00:32:38,920 --> 00:32:42,000
I probably want to stop
running it at some point.

791
00:32:42,000 --> 00:32:45,140
And so I can say
streaming query dot stop,

792
00:32:45,140 --> 00:32:46,619
we'll stop running the thing.

793
00:32:46,619 --> 00:32:48,480
Let me just run it
again. Let me show

794
00:32:48,480 --> 00:32:50,520
you another way I can do it,
right? So that's running.

795
00:32:50,520 --> 00:32:51,939
Because sometimes you forget to

796
00:32:51,939 --> 00:32:53,219
save it in a variable, right?

797
00:32:53,219 --> 00:32:56,780
And so the other ways you
can say spark dot streams.

798
00:32:56,780 --> 00:32:59,800
And then I can say
specifically the active ones.

799
00:32:59,800 --> 00:33:01,860
That'll give me a list.

800
00:33:01,860 --> 00:33:04,060
I can get the first item,
and I can say stop.

801
00:33:04,060 --> 00:33:06,460
So I can d row and I can stop
whatever I want to stop,

802
00:33:06,460 --> 00:33:09,000
or if I have a variable, I
think that's preferable.

803
00:33:09,000 --> 00:33:10,919
All right, so that
was one app, right?

804
00:33:10,919 --> 00:33:12,740
We built a shark alert app.

805
00:33:12,740 --> 00:33:17,600
All right. Any other questions
about the Shark alert app?

806
00:33:19,160 --> 00:33:28,029
Yeah, right here. Oh, what
are some other formats?

807
00:33:28,029 --> 00:33:29,309
Yeah, that's an excellent
question, right?

808
00:33:29,309 --> 00:33:32,609
So I could be like
a FCA producer,

809
00:33:32,609 --> 00:33:33,970
right where I consume
from one stream

810
00:33:33,970 --> 00:33:35,749
and then write alert
to another one.

811
00:33:35,749 --> 00:33:38,769
I could write it to Park
files and HDFS, right?

812
00:33:38,769 --> 00:33:40,349
So maybe I'm just
trying to build

813
00:33:40,349 --> 00:33:42,290
up some dataset for
people to analyze.

814
00:33:42,290 --> 00:33:45,709
It could be, maybe like
a my SQL data base

815
00:33:45,709 --> 00:33:49,149
or it could be
like I don't know.

816
00:33:49,149 --> 00:33:51,909
I could write it to like
a socket somewhere.

817
00:33:51,909 --> 00:33:53,249
It's actually very plug. You can

818
00:33:53,249 --> 00:33:54,449
put a lot of different
things there.

819
00:33:54,449 --> 00:33:55,769
Yeah, great question. Yeah.

820
00:33:55,769 --> 00:33:58,029
It looks like
there's a follow up.

821
00:34:02,200 --> 00:34:08,759
Yeah. Oh, you're saying

822
00:34:08,759 --> 00:34:13,839
that if I said it to like if
I had like a shorter time,

823
00:34:13,839 --> 00:34:17,079
like, would it still
be able to keep up or

824
00:34:25,310 --> 00:34:27,689
Oh, yeah. And in this case,

825
00:34:27,689 --> 00:34:29,249
I'm getting a bunch
of messages at once.

826
00:34:29,249 --> 00:34:33,149
So in this case, I had three
messages at once, right?

827
00:34:33,149 --> 00:34:35,089
Absolutely. Yep.
It's time waiting.

828
00:34:35,089 --> 00:34:37,889
I mean, what we see is that
for each of these batch,

829
00:34:37,889 --> 00:34:39,509
is actually running a
different spark job,

830
00:34:39,509 --> 00:34:41,049
and it's like, Well,

831
00:34:41,049 --> 00:34:42,649
it wants to do it quickly enough

832
00:34:42,649 --> 00:34:44,129
so we quickly know about
the sharks, right?

833
00:34:44,129 --> 00:34:45,609
But also, it's like
trying to batch

834
00:34:45,609 --> 00:34:46,950
up enough, so it's efficient.

835
00:34:46,950 --> 00:34:49,509
Yeah, a great question.
Yeah, other questions people

836
00:34:49,509 --> 00:34:52,569
have. All right. C.

837
00:34:52,569 --> 00:34:54,669
So for the next app,
we're going to say,

838
00:34:54,669 --> 00:34:58,449
let's build an
animal counter app.

839
00:34:58,449 --> 00:35:02,029
And that's a little
bit this is the case

840
00:35:02,029 --> 00:35:03,449
where we have an
advantage of using

841
00:35:03,449 --> 00:35:05,549
Spark over just Pre Python code.

842
00:35:05,549 --> 00:35:07,534
Right? When I come back here,

843
00:35:07,534 --> 00:35:09,700
to where I was
writing this stuff,

844
00:35:09,700 --> 00:35:11,579
I chose my key to be the beach.

845
00:35:11,579 --> 00:35:14,720
And remember that when I do a
send message with that key,

846
00:35:14,720 --> 00:35:16,399
it will hash partition on that.

847
00:35:16,399 --> 00:35:18,380
And so what the animal producer

848
00:35:18,380 --> 00:35:19,519
right now is it's
bringing together

849
00:35:19,519 --> 00:35:23,020
related data when I have two
rows with the same beach.

850
00:35:23,020 --> 00:35:25,100
That means if I have one animal

851
00:35:25,100 --> 00:35:26,640
that's seen in
different beaches,

852
00:35:26,640 --> 00:35:29,479
that data will be split across
different topics, right?

853
00:35:29,479 --> 00:35:32,509
And if I was just writing like
a Python consumer, right?

854
00:35:32,509 --> 00:35:34,079
I wouldn't be very easy to

855
00:35:34,079 --> 00:35:35,519
count the animals in one place.

856
00:35:35,519 --> 00:35:37,240
I could get two
different sub counts

857
00:35:37,240 --> 00:35:38,959
and two different consumers.

858
00:35:38,959 --> 00:35:40,759
Now, Spark is great at

859
00:35:40,759 --> 00:35:42,980
hash partitioning and bring
related data together.

860
00:35:42,980 --> 00:35:44,659
So it doesn't share
what Kafka does.

861
00:35:44,659 --> 00:35:46,699
It's try to do that hash
partitioning as an extra step,

862
00:35:46,699 --> 00:35:48,399
bring together related data,

863
00:35:48,399 --> 00:35:51,659
and then tell me how many
animals there are of each type,

864
00:35:51,659 --> 00:35:55,000
right? So we have animals.

865
00:35:55,000 --> 00:35:58,859
And we can, if we want to,

866
00:35:58,859 --> 00:36:00,379
we can group them, right?

867
00:36:00,379 --> 00:36:05,669
We can group them by the
animal type. All right.

868
00:36:05,669 --> 00:36:08,069
Then I can say, let's
count them, right?

869
00:36:08,069 --> 00:36:10,789
So I have that count
there. And then

870
00:36:10,789 --> 00:36:12,209
I can do all the
stuff I did before.

871
00:36:12,209 --> 00:36:15,009
Let me just copy it because
I don't want to type it all.

872
00:36:15,009 --> 00:36:17,869
I can do all the right
stream stuff to try

873
00:36:17,869 --> 00:36:21,289
to get it onto the
stream, right? All right.

874
00:36:21,289 --> 00:36:24,309
And I may have a streaming
query like before,

875
00:36:24,309 --> 00:36:27,130
streaming query will
be all the stuff.

876
00:36:27,130 --> 00:36:28,369
Run to a little issue now

877
00:36:28,369 --> 00:36:29,809
and this give me an
opportunity to talk

878
00:36:29,809 --> 00:36:32,589
about the output
mode. All right?

879
00:36:32,589 --> 00:36:34,209
So let's see what
it says down here.

880
00:36:34,209 --> 00:36:35,729
A Penn output mode is not

881
00:36:35,729 --> 00:36:38,149
supported when there's a
streaming aggregation.

882
00:36:38,149 --> 00:36:39,769
So exception with water marks.

883
00:36:39,769 --> 00:36:40,830
We'll talk about watermarks.

884
00:36:40,830 --> 00:36:43,649
But in general, I cannot
do the pen type when I'm

885
00:36:43,649 --> 00:36:46,199
having a group by

886
00:36:46,199 --> 00:36:48,439
or another kind of
aggregate. Why is that?

887
00:36:48,439 --> 00:36:52,239
Well, remember that a pend
means I get some new data

888
00:36:52,239 --> 00:36:53,899
and I write something out

889
00:36:53,899 --> 00:36:56,459
without changing
previously what I put out.

890
00:36:56,459 --> 00:36:58,159
Something I filter that's fine.

891
00:36:58,159 --> 00:37:01,140
In this case, let's say I
told you there's ten sharks,

892
00:37:01,140 --> 00:37:03,319
and now I see another shark.

893
00:37:03,319 --> 00:37:06,139
I can't go back and
change it to 11, right?

894
00:37:06,139 --> 00:37:08,899
And so in that case, I
cannot use that output mode.

895
00:37:08,899 --> 00:37:11,320
There's some other output
modes. One is complete.

896
00:37:11,320 --> 00:37:14,120
And that means that every
time we see some new animals,

897
00:37:14,120 --> 00:37:16,419
we're going to dump out a count

898
00:37:16,419 --> 00:37:18,779
of every animal
we've seen so far.

899
00:37:18,779 --> 00:37:21,819
Let's do that. So I'm
going to run that thing.

900
00:37:21,819 --> 00:37:25,219
And there's 200 tasks
running right now.

901
00:37:25,219 --> 00:37:27,639
What did I infer based

902
00:37:27,639 --> 00:37:30,100
on the fact that
there's 200 tasks?

903
00:37:35,630 --> 00:37:38,610
It's running another
patch, another 200.

904
00:37:38,610 --> 00:37:45,409
Yeah, right here. When
it's trying to do what?

905
00:37:45,409 --> 00:37:48,749
Yeah, we saw that for y,

906
00:37:48,749 --> 00:37:50,690
we saw for group, we
do hash partitioning,

907
00:37:50,690 --> 00:37:53,089
and then sometimes we
end up with 200, right?

908
00:37:53,089 --> 00:37:55,009
And the reason right

909
00:37:55,009 --> 00:37:56,649
is that we have to bring
related data together, right?

910
00:37:56,649 --> 00:37:58,709
So you do like a
hash on the key,

911
00:37:58,709 --> 00:38:00,989
and then moy the
partition count.

912
00:38:00,989 --> 00:38:03,989
And any kind of partition count
would be logically valid,

913
00:38:03,989 --> 00:38:05,790
but it may be faster or slower.

914
00:38:05,790 --> 00:38:08,110
And by default, they do 200,

915
00:38:08,110 --> 00:38:09,469
and that's usually too many,

916
00:38:09,469 --> 00:38:11,109
but it's been okay before
because they would

917
00:38:11,109 --> 00:38:14,449
co small partitions
together, right?

918
00:38:14,449 --> 00:38:16,530
They can't do that
with Spark streaming

919
00:38:16,530 --> 00:38:19,530
because how can you say
it's a small partition?

920
00:38:19,530 --> 00:38:21,050
It's not like we
have one fixed data

921
00:38:21,050 --> 00:38:22,309
that we look at and
see how big it is.

922
00:38:22,309 --> 00:38:24,170
It's like, Well, I
do the partitioning.

923
00:38:24,170 --> 00:38:24,709
I have to decide,

924
00:38:24,709 --> 00:38:26,249
and I have to keep doing
that indefinitely, right?

925
00:38:26,249 --> 00:38:27,570
I don't know what
the future holds.

926
00:38:27,570 --> 00:38:29,250
And so it has a bad default,

927
00:38:29,250 --> 00:38:31,029
which is 200, and there's not

928
00:38:31,029 --> 00:38:33,549
a good way for them to
automatically fix it, right?

929
00:38:33,549 --> 00:38:35,509
And so that's one of the
things I want to fix.

930
00:38:35,509 --> 00:38:36,949
The other thing I
want to fix here

931
00:38:36,949 --> 00:38:38,650
is what was coming on earlier,

932
00:38:38,650 --> 00:38:41,769
I want to start counting from
the very beginning, right?

933
00:38:41,769 --> 00:38:43,789
So I'm come down here and
I'm going to stop this job.

934
00:38:43,789 --> 00:38:47,289
I'm going to say,
streaming query dot stop.

935
00:38:47,289 --> 00:38:49,229
And whatever, I'll
get some error.

936
00:38:49,229 --> 00:38:52,149
It's fine. I interrupted it.
It's fine to get an error.

937
00:38:52,149 --> 00:38:55,859
Okay, so What I'm
going to do first

938
00:38:55,859 --> 00:38:57,539
is I am going to go way back

939
00:38:57,539 --> 00:38:59,520
here to where I made my session,

940
00:38:59,520 --> 00:39:01,059
and I actually have
to configure this.

941
00:39:01,059 --> 00:39:02,999
Unfortunately, the number of

942
00:39:02,999 --> 00:39:07,039
shuffle partitions is fixed
for every single spart job.

943
00:39:07,039 --> 00:39:08,499
I have to set it
globally, right?

944
00:39:08,499 --> 00:39:10,099
So I will do that.

945
00:39:10,099 --> 00:39:17,759
I will say, I will say spark
dot squle do partitions.

946
00:39:17,759 --> 00:39:19,119
And I'm just going to have

947
00:39:19,119 --> 00:39:20,620
ten Because I only
have like one machine.

948
00:39:20,620 --> 00:39:21,759
I don't have a lot
of resources, I

949
00:39:21,759 --> 00:39:23,480
makes sense to have
a lot of partitions.

950
00:39:23,480 --> 00:39:25,659
Alright, I'm to do that. And I

951
00:39:25,659 --> 00:39:27,860
probably have to restart
all of this, actually.

952
00:39:27,860 --> 00:39:29,639
Fortunately, I'm to do that.

953
00:39:29,639 --> 00:39:31,220
Probably need to restart,

954
00:39:31,220 --> 00:39:33,570
I need to restart my broker.

955
00:39:33,570 --> 00:39:35,640
Or my producer.

956
00:39:35,640 --> 00:39:37,239
I'm going to
recreate that thing.

957
00:39:37,239 --> 00:39:40,499
I'm probably download
all that stuff again.

958
00:39:42,620 --> 00:39:45,519
Great. Let me just
head down here.

959
00:39:45,519 --> 00:39:47,119
I'm not t to rerun everything.

960
00:39:47,119 --> 00:39:49,479
I'm not going to do
the shark stuff again.

961
00:39:49,479 --> 00:39:53,339
But down here,

962
00:39:53,339 --> 00:39:55,039
I'm actually getting my
streaming data frame.

963
00:39:55,039 --> 00:40:00,449
I'm going to do all
of this. Then I

964
00:40:00,449 --> 00:40:01,729
think I'm good down
here where I can

965
00:40:01,729 --> 00:40:03,629
actually start
running this again.

966
00:40:03,629 --> 00:40:06,429
All right. So we're gonna run
it. And instead of seeing

967
00:40:06,429 --> 00:40:09,789
200 tasks of two partitions,
you saw it was like ten.

968
00:40:09,789 --> 00:40:11,309
And now we're trying
to do it like so fast,

969
00:40:11,309 --> 00:40:14,049
maybe we might even not see
it all the time, right?

970
00:40:14,049 --> 00:40:15,629
It's kind of warmed up a bit.

971
00:40:15,629 --> 00:40:17,369
Great. So it's running a lot

972
00:40:17,369 --> 00:40:19,349
faster because I chose a
better partition count.

973
00:40:19,349 --> 00:40:21,609
That's one takeaway, right?
I'm going to do that.

974
00:40:21,609 --> 00:40:23,070
And then the other comment

975
00:40:23,070 --> 00:40:25,429
was that when we
read this thing,

976
00:40:25,429 --> 00:40:27,990
we should seek back
to the beginning.

977
00:40:27,990 --> 00:40:30,189
And so when I come back here,

978
00:40:30,189 --> 00:40:31,689
there's an option for that,

979
00:40:31,689 --> 00:40:37,849
and I can say dot Option
and starting offsets.

980
00:40:37,849 --> 00:40:39,989
I want to do the
earliest, right?

981
00:40:39,989 --> 00:40:41,890
A little bit differ than
the sk to beginning,

982
00:40:41,890 --> 00:40:42,569
but it's the same thing.

983
00:40:42,569 --> 00:40:44,690
It's probably a wrapper
around that, honestly.

984
00:40:44,690 --> 00:40:46,909
I'm just going to run
through this again,

985
00:40:46,909 --> 00:40:49,809
and let's skip the sharks and

986
00:40:49,809 --> 00:40:52,729
trump down to this app.
I'm going to do that.

987
00:40:52,729 --> 00:40:54,949
Now, the first job is
actually probably big,

988
00:40:54,949 --> 00:40:56,189
because there was
a lot of data, I

989
00:40:56,189 --> 00:40:57,569
had to count up
all these things.

990
00:40:57,569 --> 00:40:59,849
But now it's caught up.
Things are running fast.

991
00:40:59,849 --> 00:41:01,449
I get a few animals at a time.

992
00:41:01,449 --> 00:41:03,809
There was one shark
between this batch and

993
00:41:03,809 --> 00:41:08,359
the last batch. All right, cool.

994
00:41:08,600 --> 00:41:10,779
The last thing I want to show is

995
00:41:10,779 --> 00:41:12,499
that maybe actually
should be running this

996
00:41:12,499 --> 00:41:17,519
again is that we have
this console on 40 40,

997
00:41:17,519 --> 00:41:20,199
and as this is running,

998
00:41:20,400 --> 00:41:23,819
I guess there's a bunch
of jobs down here.

999
00:41:23,819 --> 00:41:27,300
We have, like 11 completed
jobs, 12 completed jobs.

1000
00:41:27,300 --> 00:41:29,019
Every single batch of data runs

1001
00:41:29,019 --> 00:41:31,060
as a new, there's one active.

1002
00:41:31,060 --> 00:41:32,879
Now it's 113 jobs.

1003
00:41:32,879 --> 00:41:37,219
Every new batch is its
own Spark job, right?

1004
00:41:37,219 --> 00:41:38,579
If you keep doing
this, it runs forever,

1005
00:41:38,579 --> 00:41:40,220
and you have lots
and lots of jobs,

1006
00:41:40,220 --> 00:41:41,939
and that's just how it works.

1007
00:41:41,939 --> 00:41:43,979
Right. Cool. Any questions about

1008
00:41:43,979 --> 00:41:46,299
the programming demos
before we dive into

1009
00:41:46,299 --> 00:41:48,319
some concepts and
actually understand how

1010
00:41:48,319 --> 00:41:52,199
Spark streaming works internally.
Questions about that.

1011
00:41:53,650 --> 00:41:56,729
All right. Cool.
So let's head over

1012
00:41:56,729 --> 00:42:00,090
here to the slides for Friday.

1013
00:42:00,090 --> 00:42:01,309
We're going to kind
of an early start

1014
00:42:01,309 --> 00:42:03,409
on it, which is nice.

1015
00:42:03,409 --> 00:42:05,309
And I have a few things

1016
00:42:05,309 --> 00:42:06,849
that I want you to
walk away with.

1017
00:42:06,849 --> 00:42:08,590
One is that remember with Spark.

1018
00:42:08,590 --> 00:42:10,390
Everything is built
on these RDDs,

1019
00:42:10,390 --> 00:42:12,170
Resilient distributed data sets,

1020
00:42:12,170 --> 00:42:13,869
and the same thing
with Spark Stream.

1021
00:42:13,869 --> 00:42:15,809
I want to understand how
I have a Spark Stream,

1022
00:42:15,809 --> 00:42:18,190
how it gets broken down
into micro batches

1023
00:42:18,190 --> 00:42:20,790
and each micro batch
runs as an RDD.

1024
00:42:20,790 --> 00:42:23,229
Okay? We've already
seen that there's

1025
00:42:23,229 --> 00:42:25,829
this kind of finicky setting
about the outputs it a pen?

1026
00:42:25,829 --> 00:42:27,029
Is it complete? Something else?

1027
00:42:27,029 --> 00:42:28,289
I want do you understand
what that is in

1028
00:42:28,289 --> 00:42:31,650
more detail and how to pick
it for a given situation.

1029
00:42:32,050 --> 00:42:34,669
It turns out the vast
majority of the things

1030
00:42:34,669 --> 00:42:36,390
that you can do with
regular Spark SQL,

1031
00:42:36,390 --> 00:42:38,270
you can do with
streaming Sparks equal.

1032
00:42:38,270 --> 00:42:40,769
But there's a few
things like pivots

1033
00:42:40,769 --> 00:42:43,450
or certain types of joins
that are not supported.

1034
00:42:43,450 --> 00:42:45,369
And I want you to get

1035
00:42:45,369 --> 00:42:47,470
some intuition about why those
things aren't supported,

1036
00:42:47,470 --> 00:42:48,969
so you can kind of anticipate

1037
00:42:48,969 --> 00:42:50,709
when certain features
will be there or not.

1038
00:42:50,709 --> 00:42:52,129
And then finally,
there's a bunch

1039
00:42:52,129 --> 00:42:53,449
of optimizations I
want you to know.

1040
00:42:53,449 --> 00:42:54,689
We already saw one
of them, which

1041
00:42:54,689 --> 00:42:56,810
was dealing with the number

1042
00:42:56,810 --> 00:42:58,270
of shuffle partition configures.

1043
00:42:58,270 --> 00:42:59,849
But there's some other
stuff like how you might

1044
00:42:59,849 --> 00:43:01,750
cache when you have
certain s of joins,

1045
00:43:01,750 --> 00:43:02,969
or there's this thing called

1046
00:43:02,969 --> 00:43:05,289
a watermark that you need
to be aware of. All right.

1047
00:43:05,289 --> 00:43:06,769
So those are a few
goals for you today.

1048
00:43:06,769 --> 00:43:09,309
Let me start with distributing
this idea of a D stream.

1049
00:43:09,309 --> 00:43:12,309
A DStream and a Spark Stream
mean exactly the same thing.

1050
00:43:12,309 --> 00:43:13,809
The early literature when they

1051
00:43:13,809 --> 00:43:15,430
first introduced
streaming to Spark,

1052
00:43:15,430 --> 00:43:17,290
they published a paper where
they called it DStream.

1053
00:43:17,290 --> 00:43:19,469
So sometimes you'll see
that. Sometimes people just

1054
00:43:19,469 --> 00:43:22,810
more informally
say Spark Streams.

1055
00:43:22,810 --> 00:43:26,949
All right. So D streams
are based on RDDs,

1056
00:43:26,949 --> 00:43:29,654
and so I want to
remember how RDDs work.

1057
00:43:29,654 --> 00:43:34,300
Here's some data where
I have a list of rows,

1058
00:43:34,300 --> 00:43:36,780
and I want to filter
it down to the A's,

1059
00:43:36,780 --> 00:43:39,720
and then I want to double the
value in the second column.

1060
00:43:39,720 --> 00:43:41,799
And I can see how
I have that data,

1061
00:43:41,799 --> 00:43:44,159
I do a bunch of transformations
on it, and in the end,

1062
00:43:44,159 --> 00:43:47,359
I collect it, and I get a
list of tuples again, right?

1063
00:43:47,359 --> 00:43:49,240
That's like classic RDD stuff.

1064
00:43:49,240 --> 00:43:51,985
That's where we started
our exploration of Spark.

1065
00:43:51,985 --> 00:43:54,409
Now, I want you to think about
what happens if I do that,

1066
00:43:54,409 --> 00:43:56,230
and then there's some more data.

1067
00:43:56,230 --> 00:43:57,529
A naive way to do it is

1068
00:43:57,529 --> 00:43:58,669
that whenever there's
some new data,

1069
00:43:58,669 --> 00:44:01,569
I just rerun the
entire job, right?

1070
00:44:01,569 --> 00:44:03,529
That would not be great because

1071
00:44:03,529 --> 00:44:06,170
every time I do it I start
to be a little bit slower,

1072
00:44:06,170 --> 00:44:07,889
there's more data, and

1073
00:44:07,889 --> 00:44:10,069
I'd be repeating a
lot of work, right?

1074
00:44:10,069 --> 00:44:11,989
So I could do that. It wouldn't
be a good thing to do.

1075
00:44:11,989 --> 00:44:13,649
It'd be slow, right?

1076
00:44:13,649 --> 00:44:16,789
All right. What could we
do that would be better?

1077
00:44:16,789 --> 00:44:19,149
Well, when I have
these two rounds,

1078
00:44:19,149 --> 00:44:20,829
I could actually
have them be part of

1079
00:44:20,829 --> 00:44:22,689
the same graph, right?

1080
00:44:22,689 --> 00:44:25,700
I could say that
Hey, let's cache.

1081
00:44:25,700 --> 00:44:27,330
Those results in memory.

1082
00:44:27,330 --> 00:44:29,250
And then I'm going
to have a new RDD,

1083
00:44:29,250 --> 00:44:30,749
that's going to combine the

1084
00:44:30,749 --> 00:44:32,889
old state with some new stuff.

1085
00:44:32,889 --> 00:44:34,650
I'm going to get
some new results.

1086
00:44:34,650 --> 00:44:37,009
Right? That's the main
idea they have when they

1087
00:44:37,009 --> 00:44:39,470
introduced streaming to Spark.

1088
00:44:39,470 --> 00:44:41,449
Okay? So what did that
end up looking like?

1089
00:44:41,449 --> 00:44:42,930
I mean, on top of those RDDs,

1090
00:44:42,930 --> 00:44:45,369
we usually have data frames,
which are a bunch of rows.

1091
00:44:45,369 --> 00:44:48,110
And instead of having a
fixed sized data frame,

1092
00:44:48,110 --> 00:44:51,889
this table is continuously
growing, right?

1093
00:44:51,889 --> 00:44:54,009
And rather than to

1094
00:44:54,009 --> 00:44:56,029
treat it one row at a time,
that would be kind of slow.

1095
00:44:56,029 --> 00:44:58,050
What we do is we're going to
break it into many batches.

1096
00:44:58,050 --> 00:44:58,709
I could say, well,

1097
00:44:58,709 --> 00:45:00,389
here's a many batch,
here's a many batch.

1098
00:45:00,389 --> 00:45:03,050
And I can choose different
time thresholds.

1099
00:45:03,050 --> 00:45:04,449
Depending on how I choose that,

1100
00:45:04,449 --> 00:45:06,970
I could have you know
more smaller batches

1101
00:45:06,970 --> 00:45:08,809
or fewer bigger ones, right?

1102
00:45:08,809 --> 00:45:11,049
So I have this constantly
growing data frame

1103
00:45:11,049 --> 00:45:13,010
that's growing
down into batches,

1104
00:45:13,010 --> 00:45:14,869
and I want to do
processing on each of

1105
00:45:14,869 --> 00:45:18,329
these batches to produce
more output, right?

1106
00:45:18,329 --> 00:45:20,329
And so what I may do
is I may have a chain

1107
00:45:20,329 --> 00:45:22,849
of RDDs coming off
of each batch.

1108
00:45:22,849 --> 00:45:25,410
And And in a lot of these cases,

1109
00:45:25,410 --> 00:45:27,850
the final output will
depend on previous results.

1110
00:45:27,850 --> 00:45:29,490
So I might, I don't always,

1111
00:45:29,490 --> 00:45:31,009
but I might have kind of

1112
00:45:31,009 --> 00:45:33,029
a vertical chain
here, too, right?

1113
00:45:33,029 --> 00:45:34,629
And that's what a D
stream is, right?

1114
00:45:34,629 --> 00:45:37,949
It's like a chain of all
these RDDs that build on each

1115
00:45:37,949 --> 00:45:39,370
other and it keeps growing

1116
00:45:39,370 --> 00:45:41,489
indefinitely, right? So
we have all of those.

1117
00:45:41,489 --> 00:45:42,909
Those are what a D stream or

1118
00:45:42,909 --> 00:45:45,389
a spark stream is. All right.

1119
00:45:45,389 --> 00:45:47,849
Now, sometimes I have what

1120
00:45:47,849 --> 00:45:50,489
is called a stateless D stream.

1121
00:45:50,489 --> 00:45:51,649
And that means that each of

1122
00:45:51,649 --> 00:45:53,509
these batches can be handled
independently, right?

1123
00:45:53,509 --> 00:45:58,109
This batch does not depend on
the previous batch, right?

1124
00:45:58,109 --> 00:46:00,249
That would be stateless.

1125
00:46:00,249 --> 00:46:02,939
This one state Pub
as it does depend.

1126
00:46:02,939 --> 00:46:06,330
All right. So when they're
independent, that's stateless,

1127
00:46:06,330 --> 00:46:07,989
showing to a quick
tophat to make sure that

1128
00:46:07,989 --> 00:46:10,030
people understand what it
means for it to be stateless.

1129
00:46:10,030 --> 00:46:12,229
Bizet has a lot of implications

1130
00:46:12,229 --> 00:46:14,449
for performance. All right.

1131
00:46:14,449 --> 00:46:17,549
Let me just head over here,

1132
00:46:17,549 --> 00:46:19,870
just a moment, please.

1133
00:46:21,510 --> 00:46:29,949
All right. All right.

1134
00:46:29,949 --> 00:46:31,930
So I have a query here,
and I'm wondering

1135
00:46:31,930 --> 00:46:35,029
if it is stateless or not.

1136
00:46:57,540 --> 00:47:00,819
About 3 seconds left.

1137
00:47:31,780 --> 00:47:33,899
So most people are saying, yes,

1138
00:47:33,899 --> 00:47:35,740
it's stateless,
which is correct.

1139
00:47:35,740 --> 00:47:37,979
Let's think about how we
could reason through that.

1140
00:47:37,979 --> 00:47:41,419
Let's say that first
off, I have my stream.

1141
00:47:41,419 --> 00:47:43,600
Sometime table. It's
constantly growing.

1142
00:47:43,600 --> 00:47:45,239
I can see based on
the select line

1143
00:47:45,239 --> 00:47:47,660
it has x and y columns.

1144
00:47:47,660 --> 00:47:49,759
I've got a bunch
of rows coming in

1145
00:47:49,759 --> 00:47:51,440
and I've been putting
a bunch of output.

1146
00:47:51,440 --> 00:47:56,119
Now I have a new row, which
is x equals f equals three.

1147
00:47:56,119 --> 00:47:59,059
The question is, when
I get four plus three,

1148
00:47:59,059 --> 00:48:06,000
can I put some result without
looking back at history,

1149
00:48:06,000 --> 00:48:07,359
and answer is, yes, I can.

1150
00:48:07,359 --> 00:48:09,099
I can add four plus
three, and I can tell you

1151
00:48:09,099 --> 00:48:11,779
seven without caring about
what the history was.

1152
00:48:11,779 --> 00:48:13,319
If I need to look back at

1153
00:48:13,319 --> 00:48:15,480
the history to
produce a new output,

1154
00:48:15,480 --> 00:48:18,160
then it's stateful,
but this is stateless.

1155
00:48:18,160 --> 00:48:21,739
All right. Any question
about statelessness?

1156
00:48:21,820 --> 00:48:30,190
Yeah. W. Oh, how
could it be stateful.

1157
00:48:30,190 --> 00:48:33,969
For example, if instead of
doing a sum over each row,

1158
00:48:33,969 --> 00:48:36,090
if I had a sum over each column,

1159
00:48:36,090 --> 00:48:40,329
then I would have a rolling
count and if I got a new row,

1160
00:48:40,329 --> 00:48:41,769
I could give you a new total,

1161
00:48:41,769 --> 00:48:43,870
but I would have to remember
what it was previously.

1162
00:48:43,870 --> 00:48:45,929
That makes sense? Yeah.

1163
00:48:45,929 --> 00:48:47,969
Yeah. Thanks for
asking for an example.

1164
00:48:47,969 --> 00:48:50,169
The other questions people have?

1165
00:48:50,410 --> 00:48:55,529
All right. Cool.
Heading back here.

1166
00:48:57,190 --> 00:48:59,509
So, we have

1167
00:48:59,509 --> 00:49:01,389
all these d streams building
on top of each other,

1168
00:49:01,389 --> 00:49:03,249
and of course, the whole
goal is we want our data

1169
00:49:03,249 --> 00:49:05,369
to end up in some
other system, right?

1170
00:49:05,369 --> 00:49:06,749
That's called the
sink over here.

1171
00:49:06,749 --> 00:49:08,009
And we already saw we can do

1172
00:49:08,009 --> 00:49:09,269
the Red stream for the source,

1173
00:49:09,269 --> 00:49:10,529
the right stream for the sink.

1174
00:49:10,529 --> 00:49:12,269
And so we have all
of these RDDs in

1175
00:49:12,269 --> 00:49:12,929
the middle that are

1176
00:49:12,929 --> 00:49:14,889
maybe usually depending
on each other,

1177
00:49:14,889 --> 00:49:17,999
and they're helping me move
data one batch at a time.

1178
00:49:17,999 --> 00:49:19,589
From source to sink and

1179
00:49:19,589 --> 00:49:21,210
doing some transformations
in between.

1180
00:49:21,210 --> 00:49:22,810
On both sides, it's
high pluggable.

1181
00:49:22,810 --> 00:49:24,410
I mean, we could be
pulling data from Kafka,

1182
00:49:24,410 --> 00:49:26,850
or HTS files,
Cassandra, anything.

1183
00:49:26,850 --> 00:49:28,789
Same thing on the
sink side, right?

1184
00:49:28,789 --> 00:49:30,969
Maybe I'm pulling data
off a Kaka stream,

1185
00:49:30,969 --> 00:49:32,249
and I'm writing a bunch of

1186
00:49:32,249 --> 00:49:33,949
different Park files
every time I get,

1187
00:49:33,949 --> 00:49:35,689
like, a big batch
of data, right?

1188
00:49:35,689 --> 00:49:38,570
So it's an excellent
system for moving data

1189
00:49:38,570 --> 00:49:42,410
from point A to
point B. Oh, right.

1190
00:49:42,410 --> 00:49:44,349
And I think I have
1 minute left.

1191
00:49:44,349 --> 00:49:45,549
I don't want to start
talking about this slide

1192
00:49:45,549 --> 00:49:46,950
because it'll take
more than a minute.

1193
00:49:46,950 --> 00:49:48,469
So tell you it.

1194
00:49:48,469 --> 00:49:51,510
We'll end there. And come up
if you have any questions.

1195
00:49:51,510 --> 00:49:55,329
I'm happy to chat, and
we'll dig deeper next time.
